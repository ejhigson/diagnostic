{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Diagnostics Paper Code\n",
    "\n",
    "Code for making the figures and results tables in \"Diagnostic tests for nested sampling calculations\" ([Higson et al., 2018](https://arxiv.org/abs/1804.06406)). See the paper for a detailed explanation information about the plots and tables produced.\n",
    "\n",
    "This notebook requires `nestcheck` and its dependencies, and was run with python 3.6 (but should work with python >= 3.4). Reproducing Figure 2 also requires `getdist` (<https://github.com/cmbant/getdist>), and optionally `texunc` (<https://github.com/ejhigson/texunc>) can be used for automatically printing results tables in LaTeX format. Figure 1 was produced with `tikz` and is not included. Output plots will be saved to `plots`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate run data using PyPolyChord\n",
    "\n",
    "This section of the notebook generates nested sampling run data using [PolyChord](https://ccpforge.cse.rl.ac.uk/gf/project/polychord/)'s python interface (called PyPolyChord) - if you already have data you can skip it. Output is stored a directory called `chains` in the same folder as this notebook.\n",
    "\n",
    "N.B. this requires PyPolyChord v1.14 or higher, and it must be installed without MPI. Results are parallelised using `nestcheck`'s `parallel_apply` function (based on `concurrent.futures`) rather than MPI to allow reproducible results with seeding and generating many runs from within a Jupyter notebook. PolyChord's random number generation depends on compiler versions so, while seeding can make your results consistent for a given install, they may not exactly match the results in the paper.\n",
    "\n",
    "### Define likelihoods and priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Gaussian(object):\n",
    "\n",
    "    \"\"\"Symmetric Gaussian loglikelihood centered on the origin.\"\"\"\n",
    "\n",
    "    def __init__(self, sigma=1.0, nderived=0):\n",
    "        \"\"\"\n",
    "        Set up likelihood object's hyperparameter values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sigma: float, optional\n",
    "            Standard deviation of Gaussian (the same for each parameter).\n",
    "        nderived: int, optional\n",
    "            Number of derived parameters.\n",
    "        \"\"\"\n",
    "        self.sigma = sigma\n",
    "        self.nderived = nderived\n",
    "\n",
    "    def __call__(self, theta):\n",
    "        \"\"\"\n",
    "        Calculate loglikelihood(theta), as well as any derived parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        theta: float or 1d numpy array\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        logl: float\n",
    "            Loglikelihood\n",
    "        phi: list of length nderived\n",
    "            Any derived parameters.\n",
    "        \"\"\"\n",
    "        logl = -(np.sum(theta ** 2) / (2 * self.sigma ** 2))\n",
    "        logl -= np.log(2 * np.pi * (self.sigma ** 2)) * len(theta) / 2.0\n",
    "        return logl, [0.0] * self.nderived\n",
    "\n",
    "\n",
    "class GaussianShell(object):\n",
    "\n",
    "    \"\"\"Gaussian Shell loglikelihood centred on the origin.\"\"\"\n",
    "\n",
    "    def __init__(self, sigma=0.2, rshell=2, nderived=0):\n",
    "        \"\"\"\n",
    "        Set up likelihood object's hyperparameter values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sigma: float, optional\n",
    "            Standard deviation of Gaussian (the same for each parameter).\n",
    "        rshell: float, optional\n",
    "            Distance of shell peak from origin.\n",
    "        nderived: int, optional\n",
    "            Number of derived parameters.\n",
    "        \"\"\"\n",
    "        self.sigma = sigma\n",
    "        self.rshell = rshell\n",
    "        self.nderived = nderived\n",
    "\n",
    "    def __call__(self, theta):\n",
    "        \"\"\"\n",
    "        Calculate loglikelihood(theta), as well as any derived parameters.\n",
    "\n",
    "        N.B. this loglikelihood is not normalised.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        theta: float or 1d numpy array\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        logl: float\n",
    "            Loglikelihood\n",
    "        phi: list of length nderived\n",
    "            Any derived parameters.\n",
    "        \"\"\"\n",
    "        rad = np.sqrt(np.sum(theta ** 2))\n",
    "        logl = - ((rad - self.rshell) ** 2) / (2 * self.sigma ** 2)\n",
    "        return logl, [0.0] * self.nderived\n",
    "\n",
    "\n",
    "class Rastrigin(object):\n",
    "\n",
    "    \"\"\"Rastrigin loglikelihood as described in the PolyChord paper.\"\"\"\n",
    "\n",
    "    def __init__(self, a=10, nderived=0):\n",
    "        \"\"\"\n",
    "        Set up likelihood object's hyperparameter values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        a: float, optional\n",
    "        nderived: int, optional\n",
    "            Number of derived parameters.\n",
    "        \"\"\"\n",
    "        self.a = a\n",
    "        self.nderived = nderived\n",
    "\n",
    "    def __call__(self, theta):\n",
    "        \"\"\"\n",
    "        Calculate loglikelihood(theta), as well as any derived parameters.\n",
    "\n",
    "        N.B. this loglikelihood is not normalised.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        theta: float or 1d numpy array\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        logl: float\n",
    "            Loglikelihood\n",
    "        phi: list of length nderived\n",
    "            Any derived parameters.\n",
    "        \"\"\"\n",
    "        ftheta = self.a * len(theta)\n",
    "        for th in theta:\n",
    "            ftheta += (th ** 2) - self.a * np.cos(2 * np.pi * th)\n",
    "        logl = -ftheta\n",
    "        return logl, [0.0] * self.nderived\n",
    "\n",
    "\n",
    "class Rosenbrock(object):\n",
    "\n",
    "    \"\"\"Rosenbrock loglikelihood as described in the PolyChord paper.\"\"\"\n",
    "\n",
    "    def __init__(self, a=1, b=100, nderived=0):\n",
    "        \"\"\"\n",
    "        Define likelihood hyperparameter values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        theta: 1d numpy array\n",
    "            Parameters.\n",
    "        a: float, optional\n",
    "        b: float, optional\n",
    "        nderived: int, optional\n",
    "            Number of derived parameters.\n",
    "        \"\"\"\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.nderived = nderived\n",
    "\n",
    "    def __call__(self, theta):\n",
    "        \"\"\"\n",
    "        Calculate loglikelihood(theta), as well as any derived parameters.\n",
    "\n",
    "        N.B. this loglikelihood is not normalised.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        theta: float or 1d numpy array\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        logl: float\n",
    "            Loglikelihood\n",
    "        phi: list of length nderived\n",
    "            Any derived parameters.\n",
    "        \"\"\"\n",
    "        ftheta = 0\n",
    "        for i in range(len(theta) - 1):\n",
    "            ftheta += (self.a - theta[i]) ** 2\n",
    "            ftheta += self.b * ((theta[i + 1] - (theta[i] ** 2)) ** 2)\n",
    "        logl = -ftheta\n",
    "        return logl, [0.0] * self.nderived\n",
    "\n",
    "\n",
    "class Uniform(object):\n",
    "\n",
    "    \"\"\"Uniform prior.\"\"\"\n",
    "\n",
    "    def __init__(self, minimum=0.0, maximum=1.0):\n",
    "        \"\"\"\n",
    "        Set up prior object's hyperparameter values.\n",
    "\n",
    "        Prior is uniform in [minimum, maximum] in each parameter.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        hypercube: list of floats\n",
    "          Point coordinate on unit hypercube (in probabily space).\n",
    "          See the PolyChord papers for more details.\n",
    "        minimum: float\n",
    "        maximum: float\n",
    "        \"\"\"\n",
    "        assert maximum > minimum\n",
    "        self.maximum = maximum\n",
    "        self.minimum = minimum\n",
    "\n",
    "    def __call__(self, hypercube):\n",
    "        \"\"\"\n",
    "        Map hypercube values to physical parameter values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        hypercube: list of floats\n",
    "          Point coordinate on unit hypercube (in probabily space).\n",
    "          See the PolyChord papers for more details.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        theta: list of floats\n",
    "          Physical parameter values corresponding to hypercube.\n",
    "        \"\"\"\n",
    "        return self.minimum + (self.maximum - self.minimum) * hypercube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def get_file_root(likelihood_name, nlive, nrepeats):\n",
    "    \"\"\"Returns file_root string for runs with input settings.\"\"\"\n",
    "    file_root = likelihood_name.replace(' ', '_').lower()\n",
    "    file_root += '_2d_' + str(nlive) + 'nlive_' + str(nrepeats) + 'nrepeats'\n",
    "    return file_root\n",
    "\n",
    "# Settings\n",
    "# --------\n",
    "# run start and ends\n",
    "start_ind = 0\n",
    "end_ind = 500\n",
    "# nlive and nrepeat settings\n",
    "nlive_nrepeats_list = []\n",
    "nr_list = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "nl_list = [10, 20, 50, 200, 500, 1000]\n",
    "for nr in nr_list:\n",
    "    nlive_nrepeats_list.append((100, nr))\n",
    "for nl in nl_list:\n",
    "    nlive_nrepeats_list.append((nl, 5))\n",
    "# PolyChord settings\n",
    "# ------------------\n",
    "prior = Uniform(-10, 10)\n",
    "pc_settings_kwargs = {\n",
    "    'do_clustering': True,\n",
    "    'posteriors': False,\n",
    "    'equals': False,\n",
    "    'write_dead': True,\n",
    "    'read_resume': False,\n",
    "    'write_resume': False,\n",
    "    'write_stats': True,\n",
    "    'write_prior': False,\n",
    "    'write_live': False,\n",
    "    'base_dir': 'chains_temp',\n",
    "    'feedback': -1,\n",
    "    'cluster_posteriors': False,\n",
    "    'precision_criterion': 0.001,\n",
    "    'nlives': {}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run PyPolyChord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPolyChord\n",
    "from PyPolyChord.settings import PolyChordSettings\n",
    "import nestcheck.parallel_utils\n",
    "\n",
    "ndims = 2\n",
    "for likelihood in [Gaussian(), GaussianShell(), Rastrigin(), Rosenbrock()]:\n",
    "    for nlive, num_repeats in nlive_nrepeats_list:\n",
    "        # make list of settings objects\n",
    "        file_root = get_file_root(type(likelihood).__name__.replace('GaussianShell', 'gaussian_shell').lower(), nlive, num_repeats)\n",
    "        settings_list = []\n",
    "        for i in range(start_ind, end_ind):\n",
    "            extra_root = i + 1\n",
    "            settings = PolyChordSettings(ndims, 0, file_root=file_root,\n",
    "                                         num_repeats=num_repeats,\n",
    "                                         nlive=nlive,\n",
    "                                         **pc_settings_kwargs)\n",
    "            settings.seed = extra_root * (10 ** 6) # N.B. polychord random number generators depend on compiler\n",
    "            settings.file_root += '_' + str(extra_root)\n",
    "            settings_list.append(settings)\n",
    "        # Before running in parallel make sure base_dir and base_dir/clusters\n",
    "        # exists, as if multiple threads try to make one at the same time mkdir\n",
    "        # throws an error\n",
    "        if not os.path.exists(settings_list[0].base_dir):\n",
    "            os.makedirs(settings_list[0].base_dir)\n",
    "        if not os.path.exists(settings_list[0].base_dir + '/clusters'):\n",
    "            os.makedirs(settings_list[0].base_dir + '/clusters')\n",
    "        desc = type(likelihood).__name__ + ' nlive=' + str(nlive) + ' nrep=' + str(num_repeats)\n",
    "        # Actually run PyPolyChord\n",
    "        nestcheck.parallel_utils.parallel_apply(\n",
    "            PyPolyChord.run_polychord, settings_list,\n",
    "            func_args=(prior,),\n",
    "            func_pre_args=(likelihood, ndims, 0),\n",
    "            max_workers=None, parallel=True,\n",
    "            tqdm_kwargs={'desc': desc,\n",
    "                         'leave': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up for making plots\n",
    "\n",
    "### Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import copy\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec\n",
    "import mpl_toolkits\n",
    "import nestcheck.ns_run_utils\n",
    "import nestcheck.plots\n",
    "import nestcheck.data_processing\n",
    "import nestcheck.plots\n",
    "import nestcheck.diagnostics_tables\n",
    "import nestcheck.estimators as e\n",
    "%matplotlib inline\n",
    "np.random.seed(0)\n",
    "pd.set_option('display.width', 200)\n",
    "\n",
    "# Output settings\n",
    "# ---------------\n",
    "# Set these to match the latex\n",
    "textwidth = 6.39767 * 0.99  # make 1% smaller to ensure everything fits\n",
    "matplotlib.rc('text', usetex=True)\n",
    "matplotlib.rc('font', **{'family': 'serif', 'serif': ['Computer Modern Roman'], 'size': 10})\n",
    "\n",
    "# Define useful global variables\n",
    "# ------------------------------\n",
    "likelihood_list = ['Gaussian', 'Gaussian shell', 'Rastrigin', 'Rosenbrock']\n",
    "estimator_list = [e.logz,\n",
    "                  e.evidence,\n",
    "                  e.param_mean,\n",
    "                  functools.partial(e.param_mean, param_ind=1),\n",
    "                  e.param_squared_mean,\n",
    "                  functools.partial(e.param_cred, probability=0.5),\n",
    "                  functools.partial(e.param_cred, probability=0.84),\n",
    "                  e.r_mean,\n",
    "                  functools.partial(e.r_cred, probability=0.5),\n",
    "                  functools.partial(e.r_cred, probability=0.84)]\n",
    "\n",
    "estimator_names = [e.get_latex_name(est) for est in estimator_list]\n",
    "lims = {'Gaussian':       {r'$\\theta_\\hat{1}$':   [-2, 2],\n",
    "                           r'$\\theta_\\hat{2}$':   [-2, 2],\n",
    "                           r'$\\theta_\\hat{1}^2$': [0, 2.5],\n",
    "                           r'$|\\theta|$':  [0, 2.5]},\n",
    "        'Gaussian shell': {r'$\\theta_\\hat{1}$':   [-4, 4],\n",
    "                           r'$\\theta_\\hat{2}$':   [-4, 4],\n",
    "                           r'$\\theta_\\hat{1}^2$': [0, 8],\n",
    "                           r'$|\\theta|$':  [1, 3]},\n",
    "        'Rosenbrock':     {r'$\\theta_\\hat{1}$':   [-1, 2.5],\n",
    "                           r'$\\theta_\\hat{2}$':   [-1, 5],\n",
    "                           r'$\\theta_\\hat{1}^2$': [0, 4.5],\n",
    "                           r'$|\\theta|$':  [0, 4.5]},\n",
    "        'Rastrigin':     {r'$\\theta_\\hat{1}$':    [-2.5, 2.5],\n",
    "                           r'$\\theta_\\hat{2}$':   [-2.5, 2.5],\n",
    "                           r'$\\theta_\\hat{1}^2$': [0, 5],\n",
    "                           r'$|\\theta|$':  [0, 3]}}\n",
    "fthetas = {r'$\\theta_\\hat{1}$': lambda x: x[:, 0],\n",
    "           r'$\\theta_\\hat{2}$': lambda x: x[:, 1],\n",
    "           r'$\\theta_\\hat{1}^2$': lambda x: x[:, 0] ** 2,\n",
    "           r'$|\\theta|$': lambda x: np.sqrt(np.sum(x ** 2, axis=1))}\n",
    "labels = [r'$\\theta_\\hat{1}$',\n",
    "          r'$\\theta_\\hat{2}$',\n",
    "          r'$\\theta_\\hat{1}^2$',\n",
    "          r'$|\\theta|$']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions for batch processing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_root(likelihood_name, nlive, nrepeats):\n",
    "    \"\"\"Returns file_root string for runs with input settings.\"\"\"\n",
    "    file_root = likelihood_name.replace(' ', '_').lower()\n",
    "    file_root += '_uniform_10_dgNone_2d_' + str(nlive) + 'nlive_' + str(nrepeats) + 'nrepeats'\n",
    "    return file_root\n",
    "\n",
    "def get_results_df(likelihood_list, nlive_nrepeats_list, estimator_list, estimator_names, n_simulate, n_runs=100,\n",
    "                   summary=True, true_values_dict=None, **kwargs):\n",
    "    \"\"\"Get a big pandas multiindex data frame with results for different likelihoods, nlives and nrepeats.\"\"\"\n",
    "    results_list = []\n",
    "    assert len(estimator_list) == len(estimator_names)\n",
    "    for likelihood_name in tqdm.tqdm_notebook(likelihood_list, leave=False, desc='likelihoods'):\n",
    "        if true_values_dict is not None:\n",
    "            true_values = np.full(len(estimator_names), np.nan)\n",
    "            for i, name in enumerate(estimator_names):\n",
    "                try:\n",
    "                    true_values[i] = true_values_dict[likelihood_name][name]\n",
    "                except KeyError:\n",
    "                    pass\n",
    "        else:\n",
    "            true_values = None\n",
    "        for nlive, nrepeats in tqdm.tqdm_notebook(nlive_nrepeats_list, leave=False, desc='nlive_nrepeats'):\n",
    "            file_root = get_file_root(likelihood_name, nlive, nrepeats)\n",
    "            run_list = nestcheck.data_processing.batch_process_data(\n",
    "                [file_root + '_' + str(i) for i in range(1, n_runs + 1)],\n",
    "                func_kwargs={'errors_to_handle': ()})\n",
    "            save_name = ('cache/errors_df_' + file_root + '_' + str(len(run_list)) +\n",
    "                         'runs_' + str(n_simulate) + 'sim')\n",
    "            if kwargs.get('thread_pvalue', False):\n",
    "                save_name += '_td'\n",
    "            if kwargs.get('bs_stat_dist', False):\n",
    "                save_name += '_bd'\n",
    "            if summary:\n",
    "                df_temp = nestcheck.diagnostics_tables.run_list_error_summary(\n",
    "                    run_list, estimator_list, estimator_names, n_simulate, save_name=save_name,\n",
    "                    true_values=true_values, **kwargs)\n",
    "            else:\n",
    "                df_temp = nestcheck.diagnostics_tables.run_list_error_values(\n",
    "                    run_list, estimator_list, estimator_names, n_simulate, save_name=save_name,\n",
    "                    **kwargs)\n",
    "            new_inds = ['likelihood', 'nlive', 'nrepeats']\n",
    "            df_temp['likelihood'] = likelihood_name\n",
    "            df_temp['nlive'] = nlive\n",
    "            df_temp['nrepeats'] = nrepeats\n",
    "            order = new_inds + list(df_temp.index.names)\n",
    "            df_temp.set_index(new_inds, drop=True, append=True, inplace=True)\n",
    "            df_temp = df_temp.reorder_levels(order)\n",
    "            results_list.append(df_temp)\n",
    "    results = pd.concat(results_list)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fig 2: Triangle Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get runs for plotting\n",
    "# ---------------------\n",
    "# The same runs are used for Figures 2, 3, 4 and 5.\n",
    "nlive = 100\n",
    "nruns = 2\n",
    "nrepeats = 5\n",
    "plot_run_dict = {}\n",
    "for likelihood_name in likelihood_list:\n",
    "    file_root = get_file_root(likelihood_name, nlive, nrepeats)\n",
    "    plot_run_dict[likelihood_name] = nestcheck.data_processing.batch_process_data(\n",
    "        [file_root + '_' + str(i) for i in range(1, nruns + 1)])\n",
    "\n",
    "# Settings\n",
    "# --------\n",
    "labels = [r'$\\theta_\\hat{1}$', r'$\\theta_\\hat{2}$']\n",
    "getdist_lims = copy.deepcopy(lims)\n",
    "# override some lims\n",
    "getdist_lims['Gaussian'][r'$\\theta_\\hat{1}$'] = [-2.1, 2.1]\n",
    "getdist_lims['Gaussian'][r'$\\theta_\\hat{2}$'] = [-2.1, 2.1]\n",
    "getdist_lims['Rosenbrock'][r'$\\theta_\\hat{1}$'] = [-1.5, 3.5]\n",
    "                         \n",
    "\n",
    "# Make the plots\n",
    "# --------------\n",
    "try:\n",
    "    import getdist\n",
    "    import getdist.plots\n",
    "    for name in likelihood_list:\n",
    "        samples_list = []\n",
    "        for i, run in enumerate(plot_run_dict[name]):\n",
    "            logw = nestcheck.ns_run_utils.get_logw(run)\n",
    "            weights = np.exp(logw - logw.max())\n",
    "            weights /= np.sum(weights)\n",
    "            # remove zero weights as they can throw errors\n",
    "            inds = np.nonzero(weights)\n",
    "            samples_list.append(getdist.MCSamples(samples=run['theta'][inds, :],\n",
    "                                                  names=labels,\n",
    "                                                  weights=weights[inds],\n",
    "                                                  labels=labels,\n",
    "                                                  label='Run ' + str(i + 1)))\n",
    "        g = getdist.plots.getSubplotPlotter(width_inch=0.49 * textwidth)\n",
    "        run_colors = ['red', 'blue']\n",
    "        g.triangle_plot(samples_list, param_limits=getdist_lims[name],\n",
    "                        contour_colors=run_colors,\n",
    "                        diag1d_kwargs={'normalized': True},\n",
    "                        line_args=[{'color': col} for col in run_colors])\n",
    "        g.export('plots/triangle_' + name.replace(' ', '_') + '_' + str(nlive) + 'nlive_'\n",
    "                 + str(nrepeats) + 'nrepeats.pdf')\n",
    "except ImportError:\n",
    "    print('You need to install getdist to make this plot. See https://github.com/cmbant/getdist for more details.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fig 3: Posterior distributions with bootstrap uncertainty estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get runs for plotting\n",
    "# ---------------------\n",
    "# The same runs are used for Figures 2, 3, 4 and 5.\n",
    "nlive = 100\n",
    "nruns = 2\n",
    "nrepeats = 5\n",
    "plot_run_dict = {}\n",
    "for likelihood_name in likelihood_list:\n",
    "    file_root = get_file_root(likelihood_name, nlive, nrepeats)\n",
    "    plot_run_dict[likelihood_name] = nestcheck.data_processing.batch_process_data(\n",
    "        [file_root + '_' + str(i) for i in range(1, nruns + 1)])\n",
    "\n",
    "# Settings\n",
    "# --------\n",
    "n_simulate = 500  # 500 for paper\n",
    "npoints = 200  # 200 for paper\n",
    "\n",
    "labels = [r'$\\theta_\\hat{1}$',\n",
    "          r'$\\theta_\\hat{2}$',\n",
    "          r'$|\\theta|$']\n",
    "\n",
    "\n",
    "for name in likelihood_list:\n",
    "    print(name)\n",
    "    cache_root = 'bs_param_dists_' + name.replace(' ', '_') + '_' + str(nlive) + 'nlive_' + str(nrepeats) + 'nrepeats_' + str(n_simulate) + 'sim_' + str(npoints) + 'points'\n",
    "    fig = nestcheck.plots.bs_param_dists(plot_run_dict[name],\n",
    "                                         labels=labels,\n",
    "                                         fthetas=[fthetas[lab] for lab in labels],\n",
    "                                         ftheta_lims=[lims[name][lab] for lab in labels],\n",
    "                                         cache='cache/' + cache_root,\n",
    "                                         figsize=(textwidth, 1.2),\n",
    "                                         n_simulate=n_simulate,\n",
    "                                         rasterize_contours=True,\n",
    "                                         nx=npoints, ny=npoints)\n",
    "    # Ajust figure plot size manually for best use of latex space as\n",
    "    # plt.layout_tight() does not work with the colorbars\n",
    "    fig.subplots_adjust(left=0.03, right=0.96, bottom=0.34, top=0.98)\n",
    "    fig.savefig('plots/' + cache_root + '.pdf', dpi=300)  # only contours are rasterised so dpi does not need to be that high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fig 4 and Fig 5: Parameter logX Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get runs for plotting\n",
    "# ---------------------\n",
    "# The same runs are used for Figures 2, 3, 4 and 5.\n",
    "nlive = 100\n",
    "nruns = 2\n",
    "nrepeats = 5\n",
    "plot_run_dict = {}\n",
    "for likelihood_name in likelihood_list:\n",
    "    file_root = get_file_root(likelihood_name, nlive, nrepeats)\n",
    "    plot_run_dict[likelihood_name] = nestcheck.data_processing.batch_process_data(\n",
    "        [file_root + '_' + str(i) for i in range(1, nruns + 1)])\n",
    "\n",
    "# Settings\n",
    "# --------\n",
    "n_simulate = 500  # 500 for paper  \n",
    "npoints = 100  # 100 for paper  \n",
    "\n",
    "\n",
    "logx_min = {'Gaussian': -10,\n",
    "            'Gaussian shell': -8,\n",
    "            'Rosenbrock': -12,\n",
    "            'Rastrigin': -15}\n",
    "labels = [r'$\\theta_\\hat{1}$',\n",
    "          r'$\\theta_\\hat{2}$',\n",
    "          r'$|\\theta|$']\n",
    "\n",
    "\n",
    "for name in likelihood_list:\n",
    "    if name in ['Gaussian', 'Gaussian shell']:\n",
    "        run_list = plot_run_dict[name][:1]\n",
    "    else:\n",
    "        run_list = plot_run_dict[name]\n",
    "    cache_root = 'param_logx_diagram_' + name.replace(' ', '_') + '_' + str(nlive) + 'nlive_' + str(nrepeats) + 'nrepeats_' + str(n_simulate) + 'sim'\n",
    "    cache_root += '_' + str(npoints) + 'points'\n",
    "    if len(run_list) != 1:\n",
    "        cache_root += '_' + str(len(run_list)) + 'runs'\n",
    "    fig = nestcheck.plots.param_logx_diagram(run_list,\n",
    "                                             labels=labels,\n",
    "                                             fthetas=[fthetas[lab] for lab in labels],\n",
    "                                             ftheta_lims=[lims[name][lab] for lab in labels],\n",
    "                                             cache='cache/' + cache_root,\n",
    "                                             logx_min=logx_min[name],\n",
    "                                             npoints=npoints,\n",
    "                                             rasterize_contours=True,\n",
    "                                             figsize=(textwidth * 0.49, 5))\n",
    "    fig.subplots_adjust(left=0.19, right=0.97, bottom=0.08, top=0.995)\n",
    "    fig.savefig('plots/' + cache_root + '.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 6 and Tables 1-4: Implementation error bar chart and results tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the true values of the estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dyPolyChord.likelihoods as likelihoods\n",
    "import scipy.integrate\n",
    "\n",
    "prior_scale = 10.\n",
    "\n",
    "def integrand_z(x2, x1, pc_like):\n",
    "    return np.exp(pc_like([x1, x2])[0]) / ((2 * prior_scale) ** 2)\n",
    "\n",
    "def integrand_func_not_normed(x2, x1, pc_like, ftheta):\n",
    "    return ftheta((x1, x2)) * np.exp(pc_like([x1, x2])[0]) / ((2 * prior_scale) ** 2)\n",
    "\n",
    "\n",
    "tv_fthetas = {}\n",
    "tv_fthetas[e.get_latex_name(e.param_mean)] = lambda x: x[0]\n",
    "tv_fthetas[e.get_latex_name(functools.partial(e.param_mean, param_ind=1))] = lambda x: x[1]\n",
    "tv_fthetas[e.get_latex_name(e.param_squared_mean)] = lambda x: x[0] ** 2\n",
    "tv_fthetas[e.get_latex_name(functools.partial(e.param_squared_mean, param_ind=1))] = lambda x: x[1] ** 2\n",
    "tv_fthetas[e.get_latex_name(e.r_mean)] = lambda x: np.sqrt(x[0] ** 2 + x[1] ** 2)\n",
    "\n",
    "true_values_dict = {}\n",
    "options = {\"epsabs\": 1.49e-11, \"epsrel\": 1.49e-11, 'limit': 5000}\n",
    "for like in [likelihoods.gaussian, likelihoods.gaussian_shell, likelihoods.rastrigin, likelihoods.rosenbrock]:\n",
    "    name = like.__name__.replace('_', ' ').title().replace('Shell', 'shell')\n",
    "    print(name)\n",
    "    z = scipy.integrate.nquad(integrand_z, ranges=[(-prior_scale, prior_scale), (-prior_scale, prior_scale)],\n",
    "                              args=(like,), opts=options)\n",
    "    true_values_dict[name] = {}\n",
    "    true_values_dict[name][e.get_latex_name(e.logz)] = np.log(z[0])\n",
    "\n",
    "    for ftheta_name, ftheta in tv_fthetas.items():\n",
    "        val = scipy.integrate.nquad(integrand_func_not_normed, ranges=[(-prior_scale, prior_scale), (-prior_scale, prior_scale)],\n",
    "                                    args=(like, ftheta), opts=options)\n",
    "        true_values_dict[name][ftheta_name] = val[0] / z[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get error results data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "# --------\n",
    "nlive = 100\n",
    "nrepeats = 5\n",
    "n_runs = 500\n",
    "n_simulate = 100\n",
    "\n",
    "\n",
    "# Get data\n",
    "errors_df = get_results_df(likelihood_list, [(nlive, nrepeats)], estimator_list, estimator_names, n_simulate,\n",
    "                           n_runs=n_runs, summary=True, save=True, load=True, thread_pvalue=False,\n",
    "                           bs_stat_dist=False, true_values_dict=true_values_dict, include_rmse=True,\n",
    "                           include_true_values=True)\n",
    "\n",
    "# Format_data\n",
    "estimator_names_bar = [e.get_latex_name(est) for est in [e.logz,\n",
    "                                                         e.param_mean,\n",
    "                                                         functools.partial(e.param_mean, param_ind=1),\n",
    "                                                         e.r_mean,\n",
    "                                                         e.param_squared_mean]]\n",
    "errors_df = errors_df.xs(nrepeats, level='nrepeats').xs(nlive, level='nlive')[estimator_names_bar]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot bar chart (Fig 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make bar chart\n",
    "ratio_plot = errors_df.xs('implementation std frac', level='calculation type')\n",
    "ratio_plot = ratio_plot.reorder_levels([1, 0]).T\n",
    "fig = plt.figure(figsize=(textwidth * 0.8, 2))\n",
    "ax = fig.add_subplot(111)\n",
    "ratio_plot['value'].plot.bar(yerr=ratio_plot['uncertainty'], ax=ax)\n",
    "# Add line showing 1/sqrt(2)\n",
    "ax.axhline(2 ** (-0.5), color='black',\n",
    "           linestyle='dashed', linewidth=1)\n",
    "# ax = plt.gca()\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_ylabel('Imp St.Dev. / Values St.Dev.', labelpad=10)\n",
    "ax.legend(bbox_to_anchor=(1.02, 1), title='Likelihood')\n",
    "plt.xticks(rotation=0)\n",
    "savename = ('plots/imp_error_test_' + str(n_runs) + 'runs_' + str(n_simulate) + 'sim_' + str(nlive) + 'nlive_'+\n",
    "            str(nrepeats) + 'nrepeats.pdf')\n",
    "fig.subplots_adjust(left=0.11, right=0.7, bottom=0.14, top=0.97)\n",
    "fig.savefig(savename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make results tables\n",
    "str_map = {'true values': 'Correct Result',\n",
    "           'values mean': 'Mean Calculation Result',\n",
    "           'values std': r'Values St.Dev.\\ ',\n",
    "           'values rmse': 'Values RMSE',\n",
    "           'bootstrap std mean': r'Bootstrap St.Dev.\\ ',\n",
    "           'implementation std': r'Implementation St.Dev.\\ ',\n",
    "           'implementation std frac': r'Imp St.Dev. / Val St.Dev.\\ ',\n",
    "           r'Implementation St.Dev.\\  frac': r'Imp St.Dev./Val St.Dev.\\ ',\n",
    "           'mathrm{log}': 'log'}\n",
    "df_dict = {}\n",
    "for likelihood_name in likelihood_list:\n",
    "    df = errors_df.xs(likelihood_name, level='likelihood')\n",
    "    label = 'tab:' + likelihood_name.lower().replace(' ', '_')\n",
    "    caption = ('As in \\Cref{tab:gaussian} but for calculations using the ' + likelihood_name +\n",
    "               r' likelihood~\\eqref{equ:' + likelihood_name.lower().replace(' ', '_') + r'}.')\n",
    "    try:\n",
    "        import texunc\n",
    "        df = texunc.print_latex_df(\n",
    "            df, min_dp=1, min_dp_no_error=4, str_map=str_map, caption=caption,\n",
    "            caption_above=False, label=label, zero_dp_ints=False)\n",
    "    except ImportError:\n",
    "        pass\n",
    "    # Also store the formatted df\n",
    "    df.index = [str_map[ind] for ind in list(df.index)]\n",
    "    df_dict[likelihood_name] = df\n",
    "pd.concat(df_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figures 7 and 8:  Line plots of errors vs nlive and nrepeats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a data frame of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "# --------\n",
    "n_runs_lp = 500\n",
    "n_simulate_lp = 10\n",
    "nlive_nrepeats_list = [(nl, 5) for nl in [10, 20, 50, 100, 200, 500, 1000]]\n",
    "nlive_nrepeats_list += [(100, nr) for nr in [1, 2, 10, 20, 50, 100, 200, 500, 1000]]\n",
    "# Run plots\n",
    "results_df_in = get_results_df(likelihood_list, nlive_nrepeats_list, estimator_list, estimator_names, n_simulate_lp,\n",
    "                               n_runs=n_runs_lp, summary=True, save=True, load=True, thread_pvalue=False,\n",
    "                               bs_stat_dist=False, true_values_dict=true_values_dict, include_rmse=True,\n",
    "                               include_true_values=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make line plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FormatStrFormatter\n",
    "linestyles = ['-', '--', ':', '-.']\n",
    "x_label_map = {'nlive': '{\\sc PolyChord} number of live points', 'nrepeats': '{\\sc PolyChord} \\\\texttt{num\\_repeats}'}\n",
    "ests_for_line_plot = [e.logz, e.param_mean]\n",
    "est_savename_map = {}\n",
    "for est in ests_for_line_plot:\n",
    "    est_savename_map[e.get_latex_name(est)] = est.__name__\n",
    "for df_temp in [results_df_in.xs(5, level='nrepeats'), results_df_in.xs(100, level='nlive')]:\n",
    "    for estimator_name in [e.get_latex_name(est) for est in ests_for_line_plot]:\n",
    "        print(estimator_name)\n",
    "        fig, axes = plt.subplots(nrows=len(likelihood_list), ncols=1, sharex=True, figsize=(textwidth / 2, 6))\n",
    "        fig.subplots_adjust(hspace=0)\n",
    "        for nlike, likelihood_name in enumerate(likelihood_list):\n",
    "            ax = axes[nlike]\n",
    "            for nc, calc in enumerate(['values std', 'bootstrap std mean', 'implementation std']):\n",
    "                ser = df_temp.xs(likelihood_name, level='likelihood').xs(calc, level='calculation type')[estimator_name]\n",
    "                ser = ser.sort_index()\n",
    "                ser.xs('value', level='result type').plot.line(\n",
    "                        yerr=ser.xs('uncertainty', level='result type'),\n",
    "                        ax=ax, label=calc, linestyle=linestyles[nc])\n",
    "            ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "            if 'nlive' in df_temp.index.names and likelihood_name == 'Gaussian' and estimator_name == e.get_latex_name(e.param_mean):\n",
    "                ax.set_yticks([0, 0.05, 0.1])\n",
    "            ax.set_xscale('log')\n",
    "            ax.set_ylabel('St.Dev.')\n",
    "            title = likelihood_name.title().replace('_', ' ').replace('Shell', 'shell') + ' ' + estimator_name\n",
    "            ax.set_title(title, y=0.72)\n",
    "            # make sure the labels of plots above and below each other don't clash\n",
    "            ax.set_ylim([0, ax.get_yticks()[-1]])\n",
    "            ax.tick_params(top=True, direction='inout')\n",
    "            if nlike != 0:\n",
    "                labels = ax.get_yticks().tolist()\n",
    "                ax.set_yticks(labels[:-1])\n",
    "            if nlike == len(likelihood_list) - 1:\n",
    "                ax.set_xlabel(x_label_map[ax.get_xlabel()])\n",
    "        savename = 'plots/line'\n",
    "        if 'nlive' in df_temp.index.names:\n",
    "            savename += '_nlive'\n",
    "        elif 'nrepeats' in df_temp.index.names:\n",
    "            savename += '_nrepeats'\n",
    "        savename += '_' + str(n_runs_lp) + 'runs_' + str(n_simulate_lp) + 'sim_' + est_savename_map[estimator_name] + '.pdf'\n",
    "        # Manually adjust saving as described in https://matplotlib.org/devdocs/api/_as_gen/matplotlib.pyplot.subplots_adjust.html\n",
    "        fig.subplots_adjust(hspace=0, left=0.17, right=0.995, bottom=0.07, top=0.99)\n",
    "        fig.savefig(savename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make legend in seperate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(textwidth, 0.3))\n",
    "for i, label in enumerate(['result values', 'mean bootstrap estimate', 'implementation error']):\n",
    "    plt.plot([0,1],[-1,-1], label=label)\n",
    "plt.legend(ncol=3, loc='center')\n",
    "plt.gca().set_ylim(bottom=0)\n",
    "plt.axis('off')\n",
    "fig.savefig('plots/line_plot_legend.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figures 9, 11, 12 and 13: Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "# --------\n",
    "nlive = 1000\n",
    "nrepeats = 5\n",
    "n_runs = 500\n",
    "n_simulate = 1000\n",
    "vals_df_in = get_results_df(likelihood_list, [(nlive, nrepeats)], estimator_list, estimator_names, n_simulate, n_runs=n_runs,\n",
    "                            summary=False, save=True, load=True, thread_pvalue=True, bs_stat_dist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_df_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def hist_plot(df_in, calculation, estimator, xlim=None, figsize=(6.4, 2.5),\n",
    "                nbin=50):\n",
    "    likelihood_list = ['Gaussian', 'Gaussian shell', 'Rastrigin', 'Rosenbrock']\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=len(likelihood_list),\n",
    "                             sharex=True, sharey=True, figsize=figsize)\n",
    "    plt.subplots_adjust(wspace=0)\n",
    "    for i, like in enumerate(likelihood_list):\n",
    "        ax = axes[i]\n",
    "        df = (df_in.xs(calculation, level='calculation type')\n",
    "              .unstack(level='likelihood')[estimator])\n",
    "        # Need to set range and number of bins so all plots have same bin\n",
    "        # sizes and frequencies can be compared\n",
    "        df[like].plot.hist(ax=ax, range=(xlim[0], xlim[1]), bins=nbin,\n",
    "                           label=estimator, alpha=0.7)\n",
    "        # Add lines for quantiles\n",
    "        ax.axvline(df[like].median(), ymax=0.65, color='black',\n",
    "                   linestyle='dashed', linewidth=1)\n",
    "        ax.set_title(like.replace('Shell', 'shell') + ' ' + estimator, y=0.65)\n",
    "        if xlim is not None:\n",
    "            ax.set_xlim(xlim)\n",
    "        lab = (calculation.replace('thread ', '') .replace('bootstrap ', '').title()\n",
    "               .replace('Ks', 'KS').replace('Pvalue', '$p$-value'))\n",
    "        ax.set_xlabel(lab)\n",
    "        # remove overlappling labels on x axis\n",
    "        if i != len(likelihood_list) - 1:\n",
    "            if plt.gca().xaxis.get_majorticklocs()[-1] == xlim[1]:\n",
    "                xticks = ax.xaxis.get_major_ticks()\n",
    "                xticks[-1].label1.set_visible(False)\n",
    "        ax.tick_params(axis='y', direction='inout')\n",
    "    return fig \n",
    "\n",
    "\n",
    "xlims = {'thread ks pvalue': [0,1],\n",
    "         'thread ks distance': [0,0.3],\n",
    "         'thread earth mover distance': [0, 0.2],\n",
    "         'thread energy distance': [0, 0.4],\n",
    "         'bootstrap ks pvalue': [0,1],\n",
    "         'bootstrap ks distance': [0,1],\n",
    "         'bootstrap earth mover distance': [0, 0.25],\n",
    "         'bootstrap energy distance': [0, 0.8]}\n",
    "\n",
    "for i, est in enumerate([e.param_mean, functools.partial(e.param_mean, param_ind=1)]):\n",
    "    for calc in ['thread ks pvalue', 'bootstrap ks distance', 'bootstrap earth mover distance', 'bootstrap energy distance']:\n",
    "        print(calc)\n",
    "        fig = hist_plot(vals_df_in, calc, e.get_latex_name(est), xlims[calc], nbin=50, figsize=(textwidth, 1.4))\n",
    "        savename = 'plots/hist_' + calc.replace(' ', '_') + '_theta' + str(i + 1) + '_' + str(n_runs) + 'runs_' + str(n_simulate) + 'sim_' + str(nlive) + 'nlive_'+ str(nrepeats) + 'nrepeats'\n",
    "        savename = savename.replace('.', '_') + '.pdf'\n",
    "        fig.subplots_adjust(left=0.096, right=0.985, bottom=0.29, top=0.98)\n",
    "        fig.savefig(savename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 10: 1d KDE distributions of bootstrap sampling error estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "# --------\n",
    "nlive = 1000\n",
    "nruns = 5\n",
    "nrepeats = 5\n",
    "n_simulate = 1000\n",
    "\n",
    "# Get runs\n",
    "kde_run_dict = {}\n",
    "for likelihood_name in likelihood_list:\n",
    "    file_root = get_file_root(likelihood_name, nlive, nrepeats)\n",
    "    kde_run_dict[likelihood_name] = nestcheck.data_processing.batch_process_data([file_root + '_' + str(i) for i in range(1, nruns + 1)])\n",
    "\n",
    "estimator_list_1dkde = [e.logz,\n",
    "                        e.param_mean,\n",
    "                        functools.partial(e.param_mean, param_ind=1)]\n",
    "estimator_names_1dkde = [e.get_latex_name(est) for est in estimator_list_1dkde]\n",
    "    \n",
    "bs_dict = {}\n",
    "for likelihood_name in likelihood_list:\n",
    "    bs_dict[likelihood_name] = nestcheck.diagnostics_tables.bs_values_df(\n",
    "        kde_run_dict[likelihood_name], estimator_list_1dkde, estimator_names_1dkde, n_simulate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for likelihood_name in likelihood_list:\n",
    "    print(likelihood_name)\n",
    "    bs_df = bs_dict[likelihood_name].iloc[[2, 3]]\n",
    "    fig = nestcheck.plots.kde_plot_df(\n",
    "        bs_df, figsize=(textwidth * 0.5, 1.2), num_xticks=2)\n",
    "    fig.subplots_adjust(left=0.03, right=0.97, bottom=0.35, top=0.99)\n",
    "    fig.savefig('plots/1dkde_' + likelihood_name.replace(' ', '_') + '_' + str(nlive) + 'nlive_' + str(nrepeats) + 'nrepeats_' + str(n_simulate) + 'sim.pdf')"
   ]
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {
    "0d5a5c3587954400b5cacf27e95fb2ca": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "15c90a7dbb9c4c408558c34c3aafca6d": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "1b168ce8e2ae456b969317f2be57d9c8": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    },
    "1d7d12616d2f4517b341f649848bc226": {
     "views": [
      {
       "cell_index": 13
      }
     ]
    },
    "349f2f1e82c24ba2999096e6e64c5d47": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "3e32afd63571447088990de120db071d": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    },
    "42e9049bcaee45c7a3842e94e6df35b5": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "4531a2ec0dd24f9b8573b74731f05222": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    },
    "4c119b207fa94a3d9b1f3b7f8f4bd457": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "569449d7184241baa585ff7b53f35af9": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    },
    "5eb2d8b88e894e18a9ffbe0eed17b998": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "6e53654aa8a544aa9720960498265199": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "77111c0c038b4933bf3891ce79872348": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "798d6f2d36d8463291bc99f88afdb368": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "7e8b20ac815f425a84edf7dee6bdc02a": {
     "views": [
      {
       "cell_index": 15
      }
     ]
    },
    "82b2514ce4e642a0b6af927243db004c": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "862f3e22ce9844ed99e3cace0d7f9f2e": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "a0705a7a88ba47ef81840cd51cda1234": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "a64713e83fbf44f29e2c4b7a04d2cac9": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "a66271ed443b4a7f99c0713921a9af3b": {
     "views": [
      {
       "cell_index": 13
      }
     ]
    },
    "a9088dff59904535a624e2b38bd997f5": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "a9a6958cd4784384a9f6c23a3dfae616": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "a9acf8f5b15c48e4b61bc57311dea4c9": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    },
    "b4fa32a467b84214b0215ff84926b3b0": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "bb8048532f8044559823e6e5bc500d7c": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "c778508297e4446db16de3d7a0d8c054": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "c96cfdf579644f4eaed59373c1e51244": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "d0a735342e7b404f8bb225bb117687bf": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "d732389a92354479bb7f52339b995eab": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "e291e6ffa9a345039fd555e364d5fc44": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "e29a5f0a53fd4a51a94a5ebdc6888f91": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    },
    "e69d501ddf764572ace5a3e0116dc306": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "f3331b6766924ad1bd6f63e0990666d5": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}